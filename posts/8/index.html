
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Woodstock Blog</title>
  <meta name="author" content="Charlie Brown">

  
  <meta name="description" content="Question link Let&rsquo;s say I gave you a long String and I wanted you to tell me the most common word in that String. How would you do that? Follow &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://okckd.github.io/posts/8">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Woodstock Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-52495723-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Woodstock Blog</a></h1>
  
    <h2>What's the good of living if you don't try a few things?</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:okckd.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/09/most-frequent-word-from-book/">[Question] Most Frequent Word From a Book</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-09T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/09/most-frequent-word-from-book/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/09/most-frequent-word-from-book/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.careercup.com/question?id=5715664853532672">link</a></p>

<blockquote><p>Let&rsquo;s say I gave you a long String and I wanted you to tell me the most common word in that String. How would you do that?</p>

<p>Follow-up: how about if I gave you the entire works of Alexandre Dumas, one of the most prolific authors in history. How would your solution work? How could you change it to solve this more specific problem?</p></blockquote>

<h3>Solution</h3>

<p>There are 2 solutions. Either <strong>HashMap</strong> or <strong>Trie</strong>. It&rsquo;s easy to think of first, but remember that Trie is designed to do this kind of job.</p>

<blockquote><p>A trie will be more useful in the second situation where you will have millions of words. You can have a counter at every node to see its frequency</p></blockquote>

<p><strong>Now the follow up</strong>. For big data problems, we can always do <strong>divide and conquer</strong> by hash value.</p>

<p>Alternatively, the comment by <a href="http://www.careercup.com/question?id=5715664853532672">Prince</a> mentioned how to solve with <strong>Map Reduce</strong>:</p>

<blockquote><p>Instead of loading it as a string first I would stream data so that I avoid memory spike. Further our map size might increases as new words are added to it. Also, we can use map reduce type job were we create map&lt;word, frequency> of each play in parallel and later join/collapse them this will reduce the time it will take to get result.</p>

<p>Common phrase should be no different then above algorithm. However we need to rebuild our index with &lt;phase, frequency></p></blockquote>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/09/match-triplet-with-reverse-order/">[Question] Match Triplet With Reverse Order</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-09T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/09/match-triplet-with-reverse-order/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/09/match-triplet-with-reverse-order/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.careercup.com/question?id=11655778">link</a></p>

<blockquote><p>Find the substring of length 3 which is present in the reverse order from the string.</p>

<p>Ex: if the string is abcdcba (cba is the reverse of abc) so we should return cba.</p></blockquote>

<h3>Solution</h3>

<ol>
<li><p><strong>HashMap (recommended)</strong>. Hash all substrings of length 3. O(n). Look up all reverse substrings of length 3 in this hash set. O(n) time and O(n) space.</p></li>
<li><p><strong>KMP Algo</strong>. Take every substring of length 3. Reverse it and find it in the input using KMP. O(n<sup>2</sup>) time and O(1) space.</p></li>
<li><p><strong>Build suffix tree</strong> of height 3. Then in reverse order, check triplets.</p></li>
</ol>


<p>The 3 solutions above all work well. Pick the one you love.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/09/big-data-top-k-frequency-3/">[Design] Big Data - Top K Frequency 3</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-09T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/09/big-data-top-k-frequency-3/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/09/big-data-top-k-frequency-3/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://stackoverflow.com/a/3262855">link</a></p>

<blockquote><p>The input is an endless stream of English words or phrases (we refer them as tokens).</p>

<p>Output top N tokens we have seen so far (from all the tokens we have seen!)</p></blockquote>

<h3>Analysis</h3>

<p>We will discuss the following details of implementation and optimization.</p>

<ol>
<li>String into Integer</li>
<li>Data Storage</li>
<li>Process Incoming Streams</li>
<li>Save result</li>
</ol>


<h4>1. String into Integer</h4>

<p>This is a nice trick that improves eficiency a lot.</p>

<blockquote><p>Though there is almost infinite possible words on the Internet, but after accumulating a large set of words, the possibility of finding new words becomes lower and lower.</p>

<p>We have already found 4 million different words, and assigned a unique ID for each. This is important, because sorting and comparisons on integers is <strong>much much faster</strong> than on strings.</p></blockquote>

<h4>2. Data Storage</h4>

<blockquote><p>The system keeps archive data for every token. Basically it&rsquo;s pairs of (Token, Frequency).</p>

<p>However, the table that stores the data would be so huge such that we have to partition the table physically. One partition scheme is <strong>based on ngrams</strong> of the token. If the token is a single word, it is 1gram. If the token is two-word phrase, it is 2gram.</p></blockquote>

<p>Of course we can also divide the data by the hash value. For information on <strong>ngrams</strong>, read <strong>[Design] Terminology: n-gram</strong>.</p>

<h4>3. Process Incoming Streams</h4>

<blockquote><p>The system will absorbs incoming sentences until memory becomes fully utilized (Ya, we need a MemoryManager). After taking N sentences and storing in memory, the system pauses, and starts tokenize each sentence into words and phrases. Each token (word or phrase) is counted.</p></blockquote>

<p>This data processing logic runs as a process under Memory-Manager. The next part is another processing running concurrently.</p>

<h4>4. Save result</h4>

<blockquote><p>Meanwhile, there will be another process that is activated once it finds any disk file generated by the system, then start merging it. Since the disk file is sorted, merging would take <strong>a similar process like merge sort</strong>.</p></blockquote>

<p>There is <a href="http://stackoverflow.com/a/3262855">some more steps</a> afterwards, but they&rsquo;re trivial. I have listed out the basic steps for processing large stream of incoming data (as string), and how to find out the Top K keywords.</p>

<p>I suggest you read previous <strong>[Design] Big Data &ndash; Top k Frequency</strong> posts before reading this.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/09/big-data-real-time-top-k/">[Design] Big Data - Real Time Top K</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-09T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/09/big-data-real-time-top-k/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/09/big-data-real-time-top-k/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://stackoverflow.com/questions/10189685/realtime-tracking-of-top-100-twitter-words-per-min-hour-day">link</a></p>

<blockquote><p>Given a continuous twitter feed, design an algorithm to return the 100 most
frequent words used at this minute, this hour and this day.</p></blockquote>

<h3>Analysis</h3>

<p>This is a frequent and useful problem for companies like Google and Twitter.</p>

<p>The first solution below is <strong>an approximation method</strong> which select keywords that occur more than a certain threshold.</p>

<p>The second solution is <strong>more accurate</strong> but RAM-intensive.</p>

<h3>Lossy Count</h3>

<p><strong>Solution 1 is a modified version of <a href="http://stackoverflow.com/a/8033083">Lossy Count</a></strong>. The detailed steps are explained <a href="http://stackoverflow.com/a/3260905">here</a>:</p>

<blockquote><p>Start with an empty map (red-black tree). The keys will be search terms, and the values will be a counter for the term.</p>

<ol>
<li><p>Look at each item in the stream.</p></li>
<li><p>If the term exists in the map, increment the associated counter.</p></li>
<li><p>Otherwise, if the map has fewer candidates than you&rsquo;re looking for, add it to the map with a count of one.</p></li>
<li><p>However, if the map is &ldquo;full&rdquo;, decrement the counter in each entry. If any counter reaches zero during this process, remove it from the map.</p></li>
</ol>
</blockquote>

<p><a href="http://www.cse.ust.hk/vldb2002/VLDB2002-proceedings/slides/S10P03slides.pdf">This slide show</a> explains <strong>Lossy Count</strong>, which is to divide input data into chunks. Then count elements and decrease counter by 1 after each chunk.</p>

<p><strong>Note that the result is NOT the top frequency items</strong>. Instead, the final results are <strong>order-dependent</strong>, giving heavier weight to the counts processed last. It maybe helpful in some cases, cuz we want to check the latest trend. However, if we want more accurate top keywords for all data, we will <strong>do a second pass over the log data</strong>.</p>

<p>Now let&rsquo;s discuss the threshold. Use &ldquo;aaabcd&rdquo; and map size = 2 as example. &lsquo;a&rsquo; will be inserted into map with occurance = 3. Then &lsquo;b&rsquo; is inserted, and removed. &lsquo;c&rsquo; is inserted, and removed. &rsquo;d&#8217; is inserted. Since we always decrease 1 at each step, &lsquo;a&rsquo; should only have occurance of 1 at the end. As explained <a href="http://stackoverflow.com/a/3260905">here</a>:</p>

<blockquote><p>If we limit the map to 99 entries, we are guaranteed to find any term that occurs more than 1/(1 + 99) (1%) of the time.</p></blockquote>

<p>We change the size of the map to change the threshold. The occurance of in the final result does not matter.</p>

<h3>Solution 2</h3>

<p>The lossy count does not actually produce the hourly, daily and monthly result accurately. Solution 2 will discuss how we deal with retiring old data in an accurate way.</p>

<p>Suggested by <a href="http://stackoverflow.com/a/3260768">this answer</a>, <strong>we keep a 30-day list for each keyword</strong>, that counts the daily occurance. This list is FIFO. When we remove and insert a new counter value, we update monthly total.</p>

<p>Alaternatively, <a href="http://stackoverflow.com/a/10190836">this answer</a> suggests keeping 1440 (24 * 60) HashMaps, each storing the information for one minute. <strong>And another 2 HashMap for the rolling total for the past hour, and past day</strong>.</p>

<blockquote><p>You need an array of 1440 (24*60) word+count hash maps organized the way that you describe; these are your minute-by-minute counts. You need two additional hash maps &ndash; for the rolling total of the hour and the day.</p>

<p>Define two operations on hash maps &ndash; add and subtract, with the semantic of merging counts of identical words, and removing words when their count drops to zero.</p>

<p>Each minute you start a new hash map, and update counts from the feed. At the end of the minute, you place that hash map into the array for the current minute, add it to the rolling total for the hour and for the day, and then subtract the hash map of an hour ago from the hourly running total, and subtract the hash map of 24 hours ago from the daily running total.</p></blockquote>

<p>This is a very good solution, which I would recommend as the standard solution to this &ldquo;Real Time Top k&rdquo; problem.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/08/number-of-distinct-substrings/">[Google] Number of Distinct Substrings</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-08T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/08/number-of-distinct-substrings/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/08/number-of-distinct-substrings/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.quora.com/Given-a-string-how-do-I-find-the-number-of-distinct-substrings-of-the-string">link</a></p>

<blockquote><p>Given a string, find the number of distinct substrings of the string. Example:</p>

<p>input = &ldquo;aaaa&rdquo;,</p>

<p>output = 4 (the 4 substrings are &ldquo;a&rdquo;, &ldquo;aa&rdquo;, &ldquo;aaa&rdquo;, &ldquo;aaaa&rdquo;)</p>

<p>input = &ldquo;abcd&rdquo;,</p>

<p>output = 10 (&ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;c&rdquo;, &ldquo;d&rdquo;, &ldquo;ab&rdquo;, &ldquo;bc&rdquo;, &ldquo;cd&rdquo;, &ldquo;abc&rdquo;, &ldquo;bcd&rdquo;, &ldquo;abcd&rdquo;)</p>

<p>input = &ldquo;banana&rdquo;,</p>

<p>output = 15 (&ldquo;a&rdquo;, &ldquo;an&rdquo;, &ldquo;ana&rdquo;, &ldquo;anan&rdquo;, &ldquo;anana&rdquo;, &ldquo;b&rdquo;, &ldquo;ba&rdquo;, &ldquo;ban&rdquo;, &ldquo;bana&rdquo;, &ldquo;banan&rdquo;, &ldquo;banana&rdquo;, &ldquo;n&rdquo;, &ldquo;na&rdquo;, &ldquo;nan&rdquo;, &ldquo;nana&rdquo;)</p></blockquote>

<p>This is also a question on <a href="http://www.spoj.com/problems/DISUBSTR/">SPOJ</a>.</p>

<h3>Solution</h3>

<p>This is a very good question, which tests Suffix tree, LCP and string manipulation knowledges.</p>

<p><strong>The solution is to build a suffix tree</strong>. This is because:</p>

<blockquote><p>If you look through the <strong><a href="http://qr.ae/6o6Nk">prefixes of each suffix</a></strong> of a string, you have covered all substrings of that string.</p></blockquote>

<p>There are 2 implementations. First one is slightly simpler.</p>

<h4>Implementation 1</h4>

<p><strong>Suffix array + LCP</strong> (longest common prefix). Take &ldquo;Banana&rdquo; as input, then the suffixes:</p>

<pre><code>0) BANANA
1) ANANA
2) NANA
3) ANA
4) NA
5) A
</code></pre>

<p>Sort it:</p>

<pre><code>5) A
3) ANA
1) ANANA
0) BANANA
4) NA
2) NANA
</code></pre>

<p>Then we start calculate number of substring (that is prefixes of suffix). After removing duplicated prefix, the count is:</p>

<pre><code>5) A - 1
3) ANA - 2
1) ANANA - 2
0) BANANA - 6
4) NA - 2
2) NANA - 2
</code></pre>

<p>Total number is:</p>

<pre><code>1 + 2 + 2 + 6 + 2 + 2 = 15
</code></pre>

<p>But wait, realize something? &ldquo;A&rdquo; is simply duplicate substring in &ldquo;ANA&rdquo;, which appers in &ldquo;ANANA&rdquo;. Keep this in mind, cuz we need to observe this in the 2nd implementation, too.</p>

<p>Finally, the total number is calculated like this:</p>

<pre><code>for each suffix
    ans += length(suffix) - LCP(suffix, previous suffix)
</code></pre>

<p>For more details, read <a href="http://qr.ae/6o6Nk">here</a>.</p>

<h4>Implementation 2</h4>

<p>Build a suffix tree, like this:</p>

<p><img class="middle" src="/assets/images/suffix-tree-banana.png"></p>

<p>Number of substrings is simply the <strong>sum of levels of each leaf</strong>. For the 3 branches of the suffix tree, number of levels are: 6, 5 and 4 respectively. Total = 15.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/08/all-number-given-decimal-scale/">[Google] Check All Numbers Given the Decimal Scale</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-08T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/08/all-number-given-decimal-scale/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/08/all-number-given-decimal-scale/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.mitbbs.com/article_t/JobHunting/32859887.html">link</a></p>

<blockquote><p>检查一个字符串是否包含k位a进制数的所有表示形式。</p>

<p>保证原字符串的所有字串都是合法的k位a进制数。</p>

<p>&ldquo;00110, a=2, k=2&rdquo; => true （包括了00，01，10，11）</p></blockquote>

<h3>Solution</h3>

<p>First find all substrings with length == k, then generate all numbers in a scale. This is not a difficult question.</p>

<p>We may want to score the substrings in a HashMap/HashSet. <strong>The hashing procedure is preferrably using <a href="http://en.wikipedia.org/wiki/Rolling_hash">Rolling hash</a></strong>.</p>

<blockquote><p>Rolling Hash</p>

<p>A rolling hash is a hash function where the input is hashed in a window that moves through the input.</p>

<p>A few hash functions allow a rolling hash to be computed very quickly—the new hash value is rapidly calculated given only the old hash value, the old value removed from the window, and the new value added to the window—similar to the way a moving average function can be computed much more quickly than other low-pass filters.</p></blockquote>

<p>Rolling Hash is commonly used in <strong><a href="http://www.geeksforgeeks.org/searching-for-patterns-set-3-rabin-karp-algorithm/">Rabin-Karp Algorithm</a></strong> to speed up strStr() string pattern matching.</p>

<p>People in the origin post &ndash; they discuss about &ldquo;<strong>slide window check</strong>&rdquo; algorithm. I do not understand what&rsquo;s the benefit of this. If you read this and would like to help me, please leave a comment. Thanks!</p>

<h3>A similar question</h3>

<p><a href="http://www.mitbbs.com/article_t/JobHunting/32860321.html">This</a> is simply the reverse of the question above:</p>

<blockquote><p>给出最短的字符串, which is used to 表示k位a进制数的所有表示形式.</p></blockquote>

<p>This question is solved using <strong><a href="http://en.wikipedia.org/wiki/De_Bruijn_sequence">De Bruijn sequence</a></strong>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/07/scheduling-jobs-with-max-cost/">[Facebook] Scheduling Jobs With Max Cost</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-07T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/07/scheduling-jobs-with-max-cost/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/07/scheduling-jobs-with-max-cost/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.glassdoor.com/Interview/Given-a-set-of-n-jobs-with-start-time-end-time-cost-find-a-subset-so-that-no-2-jobs-overlap-and-the-cost-is-maximum-QTN_440168.htm">link</a></p>

<blockquote><p>Given a set of n jobs with [start time, end time, cost], find a subset so that no 2 jobs overlap and the cost is maximum.</p>

<p>Let&rsquo;s assume the input is already sorted by start_time.</p></blockquote>

<h3>Solution</h3>

<p><a href="http://cs.stackexchange.com/a/14237">Somebody</a> mentioned <strong>Interval Graph</strong>, so check it out if you interested!</p>

<p>I am going to discuss both DP and recursive solution.</p>

<p>This question reminds me of <strong>[Question] 0-1 Knapsack Problem</strong> and <strong>[Question] Coin Change Problem</strong>, cuz the basic idea is to compare 2 conditions:</p>

<ol>
<li>include current element</li>
<li>or, not include current element</li>
</ol>


<p>A very good DP solution is presented in <a href="http://cs.stackexchange.com/a/16842">here</a>. The code below is written by me and it&rsquo;s very intuitive to read.</p>

<p>Leave me a comment if you have questions. And one more thing~ Happy new year!</p>

<h3>Code</h3>

<p>Dp</p>

<pre><code>private int dpSolution(Job[] jobs, int size) {
    int[] dp = new int[size];
    dp[size - 1] = jobs[size - 1].cost;
    // cost of last job equals to just itself
    for (int k = size - 2; k &gt;= 0; k--) {
        int next = findNextJob(jobs, k);
        int includeK = jobs[k].cost;
        if (next &lt; size) {
            includeK += dp[next];
        }
        int excludeK = dp[k + 1];
        dp[k] = Math.max(includeK, excludeK);
    }
    return dp[0];
}

private int findNextJob(Job[] jobs, int k) {
    int finishTime = jobs[k].finish;
    int next = k + 1;
    while (next &lt; jobs.length) {
        if (jobs[next].start &lt; finishTime) {
            next++;
        } else {
            break;
        }
    }
    return next;
}
</code></pre>

<p>Recursion</p>

<pre><code>public int recursiveSolution(Job[] jobs, int size, int k) {
    // max cost from (k)th job and onwards
    if (k == size) {
        return 0;
    }
    // (k)th job must not conflict with any previous job
    int next = findNextJob(jobs, k);
    int includeK = jobs[k].cost + recursiveSolution(jobs, size, next);
    int excludeK = recursiveSolution(jobs, size, k + 1);
    return Math.max(includeK, excludeK);
}

private int findNextJob(Job[] jobs, int k) {
    int finishTime = jobs[k].finish;
    int next = k + 1;
    while (next &lt; jobs.length) {
        if (jobs[next].start &lt; finishTime) {
            next++;
        } else {
            break;
        }
    }
    return next;
}
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/07/longest-common-substring/">[Question] Longest Common Substring</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-07T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/07/longest-common-substring/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/07/longest-common-substring/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Question</h3>

<p><a href="http://www.geeksforgeeks.org/longest-common-substring/">link</a></p>

<blockquote><p>Given two strings ‘X’ and ‘Y’, find the length of the longest common substring.</p>

<p>For example, if the given strings are “GeeksforGeeks” and “GeeksQuiz”, the output should be 5 as longest common substring is “Geeks”.</p></blockquote>

<h3>Solution</h3>

<p>This question is to be distinguished from <strong>[LintCode] Longest Common Subsequence</strong>.</p>

<p>The solution is DP, too. Even the code is similar. Read my code below.</p>

<p><strong>Updated on Jan 26th, 2015</strong>: there is another approach using <strong>Suffix Array</strong>, suggested by <a href="http://www.geeksforgeeks.org/suffix-tree-application-5-longest-common-substring-2/">this post</a> &ndash; but wait! Do not try to read that article, because you won&rsquo;t easily understand its explanations. I will summarize it in simple English:</p>

<ol>
<li>Concatenate String X with a &ldquo;#&rdquo; sign, and String Y with a &ldquo;$&rdquo; sign.</li>
<li>Build a suffix tree using both strings</li>
<li>find out node with <strong>both &ldquo;$&rdquo; and &ldquo;#&rdquo; as child</strong>.</li>
</ol>


<p>In the case of (X = xabxa, and Y = babxba), the branch &ldquo;abx&rdquo; would be the deepest node that qualifies. Thus the result. Code is not written.</p>

<h3>Code</h3>

<pre><code>public String LCSubstr(String s, String t) {
    int longest = 0;
    int tPos = -1;

    // dp[i][j] represents the LCSubstr ending at position i and j
    int[][] dp = new int[t.length() + 1][s.length() + 1];
    for (int i = 1; i &lt;= t.length(); i++) {
        for (int j = 1; j &lt;= s.length(); j++) {
            if (t.charAt(i - 1) == s.charAt(j - 1)) {
                dp[i][j] = dp[i - 1][j - 1] + 1;
                if (dp[i][j] &gt; longest) {
                    longest = dp[i][j];
                    tPos = i;
                }
            }
        }
    }
    return t.substring(tPos - longest, tPos);
}
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/07/P2P-technology/">[Design] P2P Technology</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-07T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/07/P2P-technology/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/07/P2P-technology/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Overview</h3>

<p><strong><a href="http://en.wikipedia.org/wiki/Peer-to-peer">Peer-to-peer</a> (P2P) networking</strong> is a distributed application architecture that partitions tasks or work loads between peers.</p>

<p>Peers are <strong>both suppliers and consumers</strong> of resources, in contrast to the traditional client-server model where communication is usually to and from a central server. A typical example of a file transfer that uses the client-server model is the <strong>File Transfer Protocol</strong> (FTP) service in which the client and server programs are distinct: the clients initiate the transfer, and the servers satisfy these requests.</p>

<p>This architecture was popularized by the file sharing system Napster, originally released in 1999.</p>

<h4>Precedure</h4>

<ol>
<li>Alice run P2P client software.

<ol>
<li>connect to Internet and get new IP address for each connection</li>
<li>register her files in P2P system</li>
<li>request &ldquo;Harry Potter&rdquo;</li>
<li>find other peers who have the copy</li>
<li>choose one and copy to her PC.</li>
<li>meanwhile, Alice is servig her files for other people</li>
</ol>
</li>
<li>Act like a server</li>
<li>Act like a client</li>
<li>User keyword to search content (like google)</li>
</ol>


<h3>P2P Types</h3>

<ol>
<li><p>Unstructured P2P: <strong>no coupling between nodes and file location</strong></p>

<ol>
<li>Napster</li>
<li>Gnutella</li>
<li>KaZaA</li>
</ol>
</li>
<li><p>Structured P2P: <strong>tight coupling between nodes and file location</strong></p>

<ol>
<li>DHT</li>
</ol>
</li>
</ol>


<h4>Napster</h4>

<p>Register at Napster server.</p>

<p>Centralized search, and P2P distributing.</p>

<h4>Gnutella</h4>

<p><strong>Decentralized</strong> design for searching:</p>

<ol>
<li>No central directory server</li>
<li>Each node maintain a list of neighbors (overlay network)</li>
</ol>


<p><strong>Search by flooding</strong>:</p>

<ol>
<li>BFS traversal.</li>
<li>Define maximum number of hops</li>
<li>Expanded-ring TTL search means to try 1 hop first, then try 2 hops, then 3&hellip;</li>
</ol>


<p>Join nodes:</p>

<ol>
<li>Use Bootstrap node to get some IP addresses</li>
<li>Join these IP, which becomes neighbors.</li>
</ol>


<p>Shortcomings:</p>

<ol>
<li>Flooding is <strong>NOT a scalable design</strong>.</li>
<li>Download may not complete.</li>
<li>Possibility of search failure, even then the resource presents.</li>
</ol>


<h4>KaZaA</h4>

<p>Combines Napster and Gnutella.</p>

<p>Each peer is a supernode or assigned to a supernode. Each supernode connects to 30~50 other supernodes. The supernode acts like a mini-Napster hub.</p>

<p>At registration, a PC connects to a supernode. If a supernode goes down, obtains updated list and elect another one.</p>

<p>Search within supernode, then in other supernodes. If found many nodes holding the file, do parallel downloading.</p>

<p>Automatic recovery if 1 server peer goes down. Use <strong>ContentHash</strong> to search.</p>

<h4>Structured P2P</h4>

<p>For Distributed HashTable services, refer to <strong>[Design] Distributed hash table</strong>.</p>

<h3>Conclusion</h3>

<ol>
<li><p>Unstructured P2P:</p>

<ol>
<li><strong>no coupling between nodes and file location</strong></li>
<li>Centralized direcotry service (except Gnutella)</li>
<li>Search by flooding (overhead)</li>
<li>Hierarchical architecture (non-scalable)</li>
</ol>
</li>
<li><p>Structured P2P:</p>

<ol>
<li><strong>tight coupling between nodes and file location</strong></li>
<li>DHT using consistent hashing (eg. Chord, and many other types)</li>
<li>A node is assigned to hold particular content</li>
<li>Search with more efficiency</li>
</ol>
</li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/06/distributed-hash-table/">[Design] Distributed Hash Table</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-06T00:00:00+08:00" pubdate data-updated="true"></time>
        
           | <a href="/blog/2015/01/06/distributed-hash-table/#disqus_thread"
             data-disqus-identifier="http://okckd.github.io/blog/2015/01/06/distributed-hash-table/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://blog.csdn.net/yfkiss/article/details/6977509">ref</a></p>

<h3>Distributed hash table</h3>

<p><strong>A <a href="http://en.wikipedia.org/wiki/Distributed_hash_table">distributed hash table</a> (DHT) is a class of a decentralized distributed system</strong> that provides a lookup service similar to a hash table. (key,value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key.</p>

<p>对于一个key/value对，DHT在分布式集群中，提供像HashTable一样的服务，例如简单快捷的存取、查询。</p>

<p><img class="middle" src="/assets/images/DHT.png"></p>

<p>DHTs form an infrastructure that can be used to build more complex services, such as anycast, cooperative Web caching, distributed file systems, domain name services, instant messaging, multicast, and also <strong>peer-to-peer file sharing</strong> and content distribution systems.</p>

<h4>Properties</h4>

<p>Unlike unstructured P2P, <strong>DHT is tightly coupled between nodes and file locations</strong>. (when request a content, directly go to the content instead of <strong>searching by flooding</strong>)</p>

<p>DHT has the following properties:</p>

<ol>
<li><p><strong>Autonomy and Decentralization</strong>: the nodes collectively form the system without any central coordination.</p></li>
<li><p><strong>Fault tolerance</strong>: the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing.</p></li>
<li><p><strong>Scalability</strong>: the system should function efficiently even with thousands or millions of nodes.</p></li>
</ol>


<h4>Building a DHT</h4>

<ol>
<li>Hash function that maps a file to a unique ID. Eg. hash(&ldquo;Harry Potter&rdquo;) &ndash;> 3912.</li>
<li>Distribute <strong>range space</strong> for all nodes in the network.</li>
<li>The desinated node stores the location of the file. (this is indirect approach)</li>
</ol>


<p><img class="middle" src="/assets/images/range-space.PNG"></p>

<h4>Search in DHT</h4>

<ol>
<li>Search query <strong>routed to the node whose range covers the file</strong>.</li>
<li>Each node would retains a <strong>routing information</strong> that is implemented in a fully distributed manner (i.e. no central point, no single point of failure).</li>
</ol>


<p>There is different hashing and routing techniques associated with DHT. The most important is <strong>Consistent Hashing</strong> and <strong>Chord Routing</strong>.</p>

<h3>Consistent Hashing</h3>

<p>&mdash;<a href="http://en.wikipedia.org/wiki/Consistent_hashing">Consistent hashing</a><strong> is a special kind of hashing such that when a hash table is resized and consistent hashing is used, </strong>only K/n keys need to be remapped__ on average, where K is the number of keys, and n is the number of slots.</p>

<h4>Motivation</h4>

<p>In most traditional hash tables, a change in the number of array slots causes <strong>nearly all keys</strong> to be remapped. Specifically, <a href="http://blog.csdn.net/sparkliang/article/details/5279393">the 3 cases below</a> can end up in a technology crisis:</p>

<ol>
<li><p>leaves/failures &ndash; 一个 cache 服务器 m down 掉了（在实际应用中必须要考虑这种情况），这样所有映射到 cache m 的对象都会失效，怎么办，需要把 cache m 从 cache 中移除，这时候 cache 是 N-1 台，映射公式变成了 hash(object)%(N-1)；</p></li>
<li><p>join &ndash; 由于访问加重，需要添加 cache ，这时候 cache 是 N+1 台，映射公式变成了 hash(object)%(N+1)</p></li>
<li><p>scalability &ndash; 由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。</p></li>
</ol>


<h4>Technique</h4>

<p>Consistent hashing is based on mapping each object to a point on the edge of a circle. The system maps each available machine to pseudo-randomly distributed points on the edge of the same circle.</p>

<ol>
<li>假定哈希key均匀的分布在一个环上</li>
<li>所有的节点也都分布在同一环上</li>
<li>每个节点只负责一部分Key，当节点加入、退出时只影响加入退出的节点和其邻居节点或者其他节点只有少量的Key受影响</li>
</ol>


<p>For a very detailed steps of consistent hashing, read <a href="http://blog.csdn.net/sparkliang/article/details/5279393">this Chinese blog</a>.</p>

<p><img class="middle" src="/assets/images/consistent-hashing.PNG"></p>

<p>In this way, 一致性Hash在node加入/离开时，不会导致映射关系的重大变化。</p>

<h3>Routing (searching)</h3>

<p><strong>Simple Routing</strong> would search successor node, and runtime is linear. These node would keep O(1) <strong>routing information</strong>, and spend O(n) time in <strong>query routing</strong>.</p>

<p>Otherwise, we make every node store ID and IP of all nodes, thus query routing takes O(1) but routing information is O(n).</p>

<p>We&rsquo;ll now discuss <strong>Chord Routing</strong>.</p>

<h4>Chord Routing</h4>

<p>Each node stores more info <strong>closely following it</strong> on the identifier circle than nodes further away. That is, the subsequent nodes at position 1, 2, 4, 8, 16, 32&hellip; (each entry is called a <strong>finger</strong>)</p>

<p><img class="middle" src="/assets/images/chord-routing.PNG"></p>

<p>为网络中每个Node分配一个唯一id（可以通过机器的mac地址做Hash），假设整个网络有N 个节点，我们可以认为这些整数首尾相连形成一个环，称之为Chord环。两个节点间的距离定义为节点间下标差，每个节点会存储一张路由表(Finger表)，表内顺时针按照离本节点2、4、8、16、32.……2i的距离选定log2N个其他节点的ip信息来记录。</p>

<p><strong>Routing information</strong> maintained at each node: O(logN).</p>

<p><strong>Query routing</strong> take O(logN) time.</p>

<h3>Join and leave in Chord</h3>

<p>It&rsquo;s very much like insertion and removal in Doubly Linked List. Read it yourself.</p>

<p><img class="middle" src="/assets/images/join-in-chord.PNG"></p>

<p>Special thanks to the online resources written by some CSDN bloggers.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/9">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/7">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section class="well">
  <h1>Categories</h1>
  <ul id="categories" class="nav nav-list">
    <li class='category'><a href='/blog/categories/cc150v4/'>cc150v4 (34)</a></li>
<li class='category'><a href='/blog/categories/cc150v5/'>cc150v5 (28)</a></li>
<li class='category'><a href='/blog/categories/design/'>design (66)</a></li>
<li class='category'><a href='/blog/categories/experience/'>experience (3)</a></li>
<li class='category'><a href='/blog/categories/facebook/'>facebook (10)</a></li>
<li class='category'><a href='/blog/categories/fundamental/'>fundamental (11)</a></li>
<li class='category'><a href='/blog/categories/google/'>google (57)</a></li>
<li class='category'><a href='/blog/categories/java-oop/'>java oop (26)</a></li>
<li class='category'><a href='/blog/categories/leetcode/'>leetcode (153)</a></li>
<li class='category'><a href='/blog/categories/leetcode-plus/'>leetcode_plus (9)</a></li>
<li class='category'><a href='/blog/categories/lintcode/'>lintcode (11)</a></li>
<li class='category'><a href='/blog/categories/ninechap/'>ninechap (16)</a></li>
<li class='category'><a href='/blog/categories/question/'>question (113)</a></li>
<li class='category'><a href='/blog/categories/string-search/'>string search (17)</a></li>
<li class='category'><a href='/blog/categories/testing/'>testing (7)</a></li>
<li class='category'><a href='/blog/categories/top-k/'>top k (7)</a></li>

  </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/02/08/unbalanced-tree-into-balanced-tree/">[Google] Transform a Unbalanced Tree Into Balanced Tree</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/generate-number-with-given-probability/">[Facebook] Generate Number With Given Probability</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/difference-internet-and-www/">[Design] Difference: Internet and the Web</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/bg-data-common-elements-lists/">[Design] Big Data - Find Common Elements in 2 Lists</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/07/reservoir-sampling/">[Question] Reservoir Sampling</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Charlie Brown -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'woodstockblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
