<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Design | Woodstock Blog]]></title>
  <link href="http://okckd.github.io/blog/categories/design/atom.xml" rel="self"/>
  <link href="http://okckd.github.io/"/>
  <updated>2015-01-10T19:36:19+08:00</updated>
  <id>http://okckd.github.io/</id>
  <author>
    <name><![CDATA[Charlie Brown]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Design] Terminology: N-gram]]></title>
    <link href="http://okckd.github.io/blog/2015/01/09/terminology-ngram/"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2015/01/09/terminology-ngram</id>
    <content type="html"><![CDATA[### n-gram

In the fields of computational linguistics and probability, an __[n-gram](http://en.wikipedia.org/wiki/N-gram)__ is a contiguous sequence of n items from a given sequence of text or speech. 

The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.

#### Example

<table border="1" width="100%" id="table5" cellpadding="3" style="border-collapse: collapse" bordercolor="#89B0D8" cellspacing="0">
				<tbody><tr>
					<td bgcolor="#D9ECFF">
					<p align="center">frequency</p></td>
					<td bgcolor="#D9ECFF">
					<p align="center">word1</p></td>
					<td bgcolor="#D9ECFF">
					<p align="center">word2</p></td>
					<td bgcolor="#D9ECFF">
					<p align="center">word3</p></td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">1419</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">the</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">same</p>
					</td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">461</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">more</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">likely</p>
					</td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">432</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">better</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">than</p>
					</td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">266</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">more</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">difficult</p>
					</td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">235</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">of</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">the</p>
					</td>
				</tr>
				<tr>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">226</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">much</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">more</p>
					</td>
					<td bgcolor="#F5F9FC">
					<p style="line-height: 150%; margin-top:0; margin-bottom:0" align="center">than</p>
					</td>
				</tr>
</tbody></table>

#### Downloadable n-grams sets for English

1. __[Google n-grams](https://catalog.ldc.upenn.edu/LDC2006T13)__, based on the web as of 2006. 
1. __[COCA n-grams](http://www.ngrams.info/intro.asp)__, based on Corpus of Contemporary American English [COCA]. 450 million words from 1990 to 2012. 

With n-grams data (2, 3, 4, 5-word sequences, with their frequency), we can carry out powerful queries offline. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top K Frequency 3]]></title>
    <link href="http://okckd.github.io/blog/2015/01/09/big-data-top-k-frequency-3/"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2015/01/09/big-data-top-k-frequency-3</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/a/3262855)

> The input is an endless stream of English words or phrases (we refer them as tokens).

> Output top N tokens we have seen so far (from all the tokens we have seen!) 

### Analysis

We will discuss the following details of implementation and optimization. 

1. String into Integer
2. Data Storage
3. Process Incoming Streams
4. Save result

#### 1. String into Integer

This is a nice trick that improves eficiency a lot. 

> Though there is almost infinite possible words on the Internet, but after accumulating a large set of words, the possibility of finding new words becomes lower and lower.

> We have already found 4 million different words, and assigned a unique ID for each. This is important, because sorting and comparisons on integers is __much much faster__ than on strings.

#### 2. Data Storage

> The system keeps archive data for every token. Basically it's pairs of (Token, Frequency). 

> However, the table that stores the data would be so huge such that we have to partition the table physically. One partition scheme is __based on ngrams__ of the token. If the token is a single word, it is 1gram. If the token is two-word phrase, it is 2gram. 

Of course we can also divide the data by the hash value. For information on __ngrams__, read __[Design] Terminology: n-gram__. 

#### 3. Process Incoming Streams

> The system will absorbs incoming sentences until memory becomes fully utilized (Ya, we need a MemoryManager). After taking N sentences and storing in memory, the system pauses, and starts tokenize each sentence into words and phrases. Each token (word or phrase) is counted. 

This data processing logic runs as a process under Memory-Manager. The next part is another processing running concurrently. 

#### 4. Save result

> Meanwhile, there will be another process that is activated once it finds any disk file generated by the system, then start merging it. Since the disk file is sorted, merging would take __a similar process like merge sort__. 

There is [some more steps](http://stackoverflow.com/a/3262855) afterwards, but they're trivial. I have listed out the basic steps for processing large stream of incoming data (as string), and how to find out the Top K keywords. 

I suggest you read previous __[Design] Big Data - Top k Frequency__ posts before reading this. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Real Time Top K]]></title>
    <link href="http://okckd.github.io/blog/2015/01/09/big-data-real-time-top-k/"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2015/01/09/big-data-real-time-top-k</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/questions/10189685/realtime-tracking-of-top-100-twitter-words-per-min-hour-day)

> Given a continuous twitter feed, design an algorithm to return the 100 most
frequent words used at this minute, this hour and this day. 

### Analysis

This is a frequent and useful problem for companies like Google and Twitter. 

The first solution below is __an approximation method__ which select keywords that occur more than a certain threshold. 

The second solution is __more accurate__ but RAM-intensive. 

### Lossy Count

__Solution 1 is a modified version of [Lossy Count](http://stackoverflow.com/a/8033083)__. The detailed steps are explained [here](http://stackoverflow.com/a/3260905): 

> Start with an empty map (red-black tree). The keys will be search terms, and the values will be a counter for the term. 
>
> 1. Look at each item in the stream.
>
> 1. If the term exists in the map, increment the associated counter.
>
> 1. Otherwise, if the map has fewer candidates than you're looking for, add it to the map with a count of one.
>
> 1. However, if the map is "full", decrement the counter in each entry. If any counter reaches zero during this process, remove it from the map.

[This slide show](http://www.cse.ust.hk/vldb2002/VLDB2002-proceedings/slides/S10P03slides.pdf) explains __Lossy Count__, which is to divide input data into chunks. Then count elements and decrease counter by 1 after each chunk. 

__Note that the result is NOT the top frequency items__. Instead, the final results are __order-dependent__, giving heavier weight to the counts processed last. It maybe helpful in some cases, cuz we want to check the latest trend. However, if we want more accurate top keywords for all data, we will __do a second pass over the log data__. 

Now let's discuss the threshold. Use "aaabcd" and map size = 2 as example. 'a' will be inserted into map with occurance = 3. Then 'b' is inserted, and removed. 'c' is inserted, and removed. 'd' is inserted. Since we always decrease 1 at each step, 'a' should only have occurance of 1 at the end. As explained [here](http://stackoverflow.com/a/3260905): 

> If we limit the map to 99 entries, we are guaranteed to find any term that occurs more than 1/(1 + 99) (1%) of the time. 

We change the size of the map to change the threshold. The occurance of in the final result does not matter. 

### Solution 2

The lossy count does not actually produce the hourly, daily and monthly result accurately. Solution 2 will discuss how we deal with retiring old data in an accurate way. 

Suggested by [this answer](http://stackoverflow.com/a/3260768), __we keep a 30-day list for each keyword__, that counts the daily occurance. This list is FIFO. When we remove and insert a new counter value, we update monthly total. 

Alaternatively, [this answer](http://stackoverflow.com/a/10190836) suggests keeping 1440 (24 * 60) HashMaps, each storing the information for one minute. __And another 2 HashMap for the rolling total for the past hour, and past day__. 

> You need an array of 1440 (24*60) word+count hash maps organized the way that you describe; these are your minute-by-minute counts. You need two additional hash maps - for the rolling total of the hour and the day.

> Define two operations on hash maps - add and subtract, with the semantic of merging counts of identical words, and removing words when their count drops to zero.

> Each minute you start a new hash map, and update counts from the feed. At the end of the minute, you place that hash map into the array for the current minute, add it to the rolling total for the hour and for the day, and then subtract the hash map of an hour ago from the hourly running total, and subtract the hash map of 24 hours ago from the daily running total.

This is a very good solution, which I would recommend as the standard solution to this "Real Time Top k" problem. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] P2P Technology]]></title>
    <link href="http://okckd.github.io/blog/2015/01/07/P2P-technology/"/>
    <updated>2015-01-07T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2015/01/07/P2P-technology</id>
    <content type="html"><![CDATA[### Overview

__[Peer-to-peer](http://en.wikipedia.org/wiki/Peer-to-peer) (P2P) networking__ is a distributed application architecture that partitions tasks or work loads between peers. 

Peers are __both suppliers and consumers__ of resources, in contrast to the traditional client-server model where communication is usually to and from a central server. A typical example of a file transfer that uses the client-server model is the __File Transfer Protocol__ (FTP) service in which the client and server programs are distinct: the clients initiate the transfer, and the servers satisfy these requests.

This architecture was popularized by the file sharing system Napster, originally released in 1999. 

#### Precedure

1. Alice run P2P client software. 
    1. connect to Internet and get new IP address for each connection
    1. register her files in P2P system
    1. request "Harry Potter"
    1. find other peers who have the copy
    1. choose one and copy to her PC.
    1. meanwhile, Alice is servig her files for other people
1. Act like a server
1. Act like a client
1. User keyword to search content (like google)

### P2P Types

1. Unstructured P2P: __no coupling between nodes and file location__
    1. Napster
    1. Gnutella
    1. KaZaA
    
1. Structured P2P: __tight coupling between nodes and file location__
    1. DHT

#### Napster

Register at Napster server.

Centralized search, and P2P distributing. 

#### Gnutella

__Decentralized__ design for searching:

1. No central directory server
1. Each node maintain a list of neighbors (overlay network)

__Search by flooding__:

1. BFS traversal. 
1. Define maximum number of hops
1. Expanded-ring TTL search means to try 1 hop first, then try 2 hops, then 3...

Join nodes:

1. Use Bootstrap node to get some IP addresses
1. Join these IP, which becomes neighbors.

Shortcomings:

1. Flooding is __NOT a scalable design__.
1. Download may not complete. 
1. Possibility of search failure, even then the resource presents.

#### KaZaA

Combines Napster and Gnutella. 

Each peer is a supernode or assigned to a supernode. Each supernode connects to 30~50 other supernodes. The supernode acts like a mini-Napster hub. 

At registration, a PC connects to a supernode. If a supernode goes down, obtains updated list and elect another one. 

Search within supernode, then in other supernodes. If found many nodes holding the file, do parallel downloading. 

Automatic recovery if 1 server peer goes down. Use __ContentHash__ to search.

#### Structured P2P

For Distributed HashTable services, refer to __[Design] Distributed hash table__. 

### Conclusion

1. Unstructured P2P: 
    1. __no coupling between nodes and file location__
    1. Centralized direcotry service (except Gnutella)
    1. Search by flooding (overhead)
    1. Hierarchical architecture (non-scalable)

1. Structured P2P: 
    1. __tight coupling between nodes and file location__
    1. DHT using consistent hashing (eg. Chord, and many other types)
    1. A node is assigned to hold particular content
    1. Search with more efficiency
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Distributed Hash Table]]></title>
    <link href="http://okckd.github.io/blog/2015/01/06/distributed-hash-table/"/>
    <updated>2015-01-06T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2015/01/06/distributed-hash-table</id>
    <content type="html"><![CDATA[[ref](http://blog.csdn.net/yfkiss/article/details/6977509)

### Distributed hash table

__A [distributed hash table](http://en.wikipedia.org/wiki/Distributed_hash_table) (DHT) is a class of a decentralized distributed system__ that provides a lookup service similar to a hash table. (key,value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key.

对于一个key/value对，DHT在分布式集群中，提供像HashTable一样的服务，例如简单快捷的存取、查询。

<img class="middle" src="/assets/images/DHT.png">

DHTs form an infrastructure that can be used to build more complex services, such as anycast, cooperative Web caching, distributed file systems, domain name services, instant messaging, multicast, and also __peer-to-peer file sharing__ and content distribution systems. 

#### Properties 

Unlike unstructured P2P, __DHT is tightly coupled between nodes and file locations__. (when request a content, directly go to the content instead of __searching by flooding__) 

DHT has the following properties: 

1. __Autonomy and Decentralization__: the nodes collectively form the system without any central coordination.

1. __Fault tolerance__: the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing.

1. __Scalability__: the system should function efficiently even with thousands or millions of nodes.

#### Building a DHT

1. Hash function that maps a file to a unique ID. Eg. hash("Harry Potter") -> 3912. 
2. Distribute __range space__ for all nodes in the network. 
3. The desinated node stores the location of the file. (this is indirect approach)

<img class="middle" src="/assets/images/range-space.PNG">

#### Search in DHT

1. Search query __routed to the node whose range covers the file__. 
2. Each node would retains a __routing information__ that is implemented in a fully distributed manner (i.e. no central point, no single point of failure). 

There is different hashing and routing techniques associated with DHT. The most important is __Consistent Hashing__ and __Chord Routing__. 

### Consistent Hashing

--[Consistent hashing](http://en.wikipedia.org/wiki/Consistent_hashing)__ is a special kind of hashing such that when a hash table is resized and consistent hashing is used, __only K/n keys need to be remapped__ on average, where K is the number of keys, and n is the number of slots. 

#### Motivation

In most traditional hash tables, a change in the number of array slots causes __nearly all keys__ to be remapped. Specifically, [the 3 cases below](http://blog.csdn.net/sparkliang/article/details/5279393) can end up in a technology crisis: 

1. leaves/failures - 一个 cache 服务器 m down 掉了（在实际应用中必须要考虑这种情况），这样所有映射到 cache m 的对象都会失效，怎么办，需要把 cache m 从 cache 中移除，这时候 cache 是 N-1 台，映射公式变成了 hash(object)%(N-1)；

1. join - 由于访问加重，需要添加 cache ，这时候 cache 是 N+1 台，映射公式变成了 hash(object)%(N+1)

1. scalability - 由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。

#### Technique

Consistent hashing is based on mapping each object to a point on the edge of a circle. The system maps each available machine to pseudo-randomly distributed points on the edge of the same circle.

1. 假定哈希key均匀的分布在一个环上
1. 所有的节点也都分布在同一环上
1. 每个节点只负责一部分Key，当节点加入、退出时只影响加入退出的节点和其邻居节点或者其他节点只有少量的Key受影响

For a very detailed steps of consistent hashing, read [this Chinese blog](http://blog.csdn.net/sparkliang/article/details/5279393).

<img class="middle" src="/assets/images/consistent-hashing.PNG">

In this way, 一致性Hash在node加入/离开时，不会导致映射关系的重大变化。

### Routing (searching)

__Simple Routing__ would search successor node, and runtime is linear. These node would keep O(1) __routing information__, and spend O(n) time in __query routing__. 

Otherwise, we make every node store ID and IP of all nodes, thus query routing takes O(1) but routing information is O(n). 

We'll now discuss __Chord Routing__.

#### Chord Routing

Each node stores more info __closely following it__ on the identifier circle than nodes further away. That is, the subsequent nodes at position 1, 2, 4, 8, 16, 32... (each entry is called a __finger__)

<img class="middle" src="/assets/images/chord-routing.PNG">

为网络中每个Node分配一个唯一id（可以通过机器的mac地址做Hash），假设整个网络有N 个节点，我们可以认为这些整数首尾相连形成一个环，称之为Chord环。两个节点间的距离定义为节点间下标差，每个节点会存储一张路由表(Finger表)，表内顺时针按照离本节点2、4、8、16、32.……2i的距离选定log2N个其他节点的ip信息来记录。

__Routing information__ maintained at each node: O(logN). 

__Query routing__ take O(logN) time. 

### Join and leave in Chord

It's very much like insertion and removal in Doubly Linked List. Read it yourself. 

<img class="middle" src="/assets/images/join-in-chord.PNG">

Special thanks to the online resources written by some CSDN bloggers. 
]]></content>
  </entry>
  
</feed>
