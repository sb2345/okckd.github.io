<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: z-top-k | Shuatiblog.com]]></title>
  <link href="http://www.shuatiblog.com/blog/categories/z-top-k/atom.xml" rel="self"/>
  <link href="http://www.shuatiblog.com/"/>
  <updated>2015-11-18T21:14:39-06:00</updated>
  <id>http://www.shuatiblog.com/</id>
  <author>
    <name><![CDATA[CodeMonkey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top k Frequency (hands-on) ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/01/09/big-data-top-k-frequency-3/"/>
    <updated>2015-01-09T00:00:00-06:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/01/09/big-data-top-k-frequency-3</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/a/3262855)

> The input is an endless stream of English words or phrases (we refer them as tokens).

> Output top N tokens we have seen so far (from all the tokens we have seen!) 

### Analysis

We will discuss the following details of implementation and optimization. 

1. String into Integer
2. Data Storage
3. Process Incoming Streams
4. Save result

#### 1. String into Integer

This is a nice trick that improves eficiency a lot. 

> Though there is almost infinite possible words on the Internet, but after accumulating a large set of words, the possibility of finding new words becomes lower and lower.

> We have already found 4 million different words, and assigned a unique ID for each. This is important, because sorting and comparisons on integers is __much much faster__ than on strings.

#### 2. Data Storage

> The system keeps archive data for every token. Basically it's pairs of (Token, Frequency). 

> However, the table that stores the data would be so huge such that we have to partition the table physically. One partition scheme is __based on ngrams__ of the token. If the token is a single word, it is 1gram. If the token is two-word phrase, it is 2gram. 

Of course we can also divide the data by the hash value. For information on __ngrams__, read __[Design] Terminology: n-gram__. 

#### 3. Process Incoming Streams

> The system will absorbs incoming sentences until memory becomes fully utilized (Ya, we need a MemoryManager). After taking N sentences and storing in memory, the system pauses, and starts tokenize each sentence into words and phrases. Each token (word or phrase) is counted. 

This data processing logic runs as a process under Memory-Manager. The next part is another processing running concurrently. 

#### 4. Save result

> Meanwhile, there will be another process that is activated once it finds any disk file generated by the system, then start merging it. Since the disk file is sorted, merging would take __a similar process like merge sort__. 

There is [some more steps](http://stackoverflow.com/a/3262855) afterwards, but they're trivial. I have listed out the basic steps for processing large stream of incoming data (as string), and how to find out the Top K keywords. 

I suggest you read previous __[Design] Big Data - Top k Frequency__ posts before reading this. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Real Time Top k ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/01/09/big-data-real-time-top-k/"/>
    <updated>2015-01-09T00:00:00-06:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/01/09/big-data-real-time-top-k</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/questions/10189685/realtime-tracking-of-top-100-twitter-words-per-min-hour-day)

> Given a continuous twitter feed, design an algorithm to return the 100 most
frequent words used at this minute, this hour and this day. 

### Analysis

This is a frequent and useful problem for companies like Google and Twitter. 

The first solution below is __an approximation method__ which select keywords that occur more than a certain threshold. 

The second solution is __more accurate__ but RAM-intensive. 

### Lossy Count (used to get an inaccurate trend) 

__Solution 1 is a modified version of [Lossy Count](http://stackoverflow.com/a/8033083)__. The detailed steps are explained [here](http://stackoverflow.com/a/3260905): 

> Start with an empty map (red-black tree). The keys will be search terms, and the values will be a counter for the term. 
>
> 1. Look at each item in the stream.
>
> 1. If the term exists in the map, increment the associated counter.
>
> 1. Otherwise, if the map has fewer candidates than you're looking for, add it to the map with a count of one.
>
> 1. However, if the map is "full", decrement the counter in each entry. If any counter reaches zero during this process, remove it from the map.

[This slide show](http://www.cse.ust.hk/vldb2002/VLDB2002-proceedings/slides/S10P03slides.pdf) explains __Lossy Count__, which is to divide input data into chunks. Then count elements and decrease counter by 1 after each chunk. 

__Note that the result is NOT the top frequency items__. Instead, the final results are __order-dependent__, giving heavier weight to the counts processed last. It maybe helpful in some cases, cuz we want to check the latest trend. However, if we want more accurate top keywords for all data, we will __do a second pass over the log data__. 

Now let's discuss the threshold. Use "aaabcd" and map size = 2 as example. 'a' will be inserted into map with occurance = 3. Then 'b' is inserted, and removed. 'c' is inserted, and removed. 'd' is inserted. Since we always decrease 1 at each step, 'a' should only have occurance of 1 at the end. As explained [here](http://stackoverflow.com/a/3260905): 

> If we limit the map to 99 entries, we are guaranteed to find any term that occurs more than 1/(1 + 99) (1%) of the time. 

We change the size of the map to change the threshold. The occurance of in the final result does not matter. 

### Solution 2

The lossy count does not actually produce the hourly, daily and monthly result accurately. Solution 2 will discuss how we deal with retiring old data in an accurate way. 

Suggested by [this answer](http://stackoverflow.com/a/3260768), __we keep a 30-day list for each keyword__, that counts the daily occurance. This list is FIFO. When we remove and insert a new counter value, we update monthly total. 

Alaternatively, [this answer](http://stackoverflow.com/a/10190836) suggests keeping 1440 (24 * 60) HashMaps, each storing the information for one minute. __And another 2 HashMap for the rolling total for the past hour, and past day__. 

> You need an array of 1440 (24*60) word+count hash maps organized the way that you describe; these are your minute-by-minute counts. You need two additional hash maps - for the rolling total of the hour and the day.

> Define two operations on hash maps - add and subtract, with the semantic of merging counts of identical words, and removing words when their count drops to zero.

> Each minute you start a new hash map, and update counts from the feed. At the end of the minute, you place that hash map into the array for the current minute, add it to the rolling total for the hour and for the day, and then subtract the hash map of an hour ago from the hourly running total, and subtract the hash map of 24 hours ago from the daily running total.

This is a very good solution, which I would recommend as the standard solution to this "Real Time Top k" problem. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[CC150v4] 20.6 Top Million from Billion ]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/09/10/top-million-from-billion/"/>
    <updated>2014-09-10T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/09/10/top-million-from-billion</id>
    <content type="html"><![CDATA[### Question

> Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. 

> Assume that the computer memory can hold all one billion numbers. 

### Solution

There're enough discussion on __Top K problems__ so far in this blog. The suggest solutions is: 

1. Sort

1. Min Heap, O(n logm) time. 

1. Quickselect algorithm. O(n) time. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top k Frequency (case analysis)]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/08/10/big-data-Top-k-freq-case-analysis/"/>
    <updated>2014-08-10T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/08/10/big-data-Top-k-freq-case-analysis</id>
    <content type="html"><![CDATA[### Question 

[link](http://dongxicheng.org/big-data/select-ten-from-billions/)

> 在海量数据中找出出现频率最高的前K个数，或者从海量数据中找出最大的前K个数，这类问题通常称为“top K”问题，

> 1. top K value
> 1. top K frequency

### Analysis

__Standard solution__ is 【分治+trie树/hash+小顶堆/quickselect】, which I covered in another post [Big Data - Top k Frequency](/blog/2014/07/25/big-data-Top-k-frequency/). Briefly it is 3 steps: 

1. 先将数据集按照hash方法分解成多个小数据集，
1. 使用trie树或者hash统计每个小数据集中的query词频，
1. 用小顶堆/quickselect求出每个数据集中出频率最高的前K个数

But, there're other senarios where different solutions may apply. Consider: 

1. Single core vs. multiple core

1. Single PC vs. multiple PC

1. Large RAM vs. limited RAM

1. Distributed system

### 1. 单机+单核+足够大内存

设每个查询词平均占8Byte，则10亿个查询词所需的内存大约是10^9*8=8G内存。如果你有这么大的内存，直接在内存中对查询词进行排序，顺序遍历找出10个出现频率最大的10个即可。这种方法简单快速，更加实用。当然，也可以先用HashMap求出每个词出现的频率，然后求出出现频率最大的10个词。

### 2. 单机+单核+受限内存

这种情况下，需要将原数据文件切割成一个一个小文件，如，采用hash(x)%M，将原文件中的数据切割成M小文件，如果小文件仍大于内存大小，继续采用hash的方法对数据文件进行切割，直到每个小文件小于内存大小，这样，每个文件可放到内存中处理。采用3.1节的方法依次处理每个小文件。

### 3. 单机+多核+足够大内存

这时可以直接在内存中实用hash方法将数据划分成n个partition，每个partition交给一个线程处理，线程的处理逻辑是同[1]节类似，最后一个线程将结果归并。

该方法存在一个瓶颈会明显影响效率，即数据倾斜，每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。解决方法是，__将数据划分成 (c x n)个partition（c>1），每个线程处理完当前partition后主动取下一个partition继续处理__，直到所有数据处理完毕，最后由一个线程进行归并。

### 4. 多机+受限内存

这种情况下，为了合理利用多台机器的资源，可将数据分发到多台机器上，每台机器采用[3]节中的策略解决本地的数据。可采用__hash + socket__方法进行数据分发。

### 5. Distributed

Top k问题很适合采用__MapReduce框架__解决，用户只需编写一个map函数和两个reduce 函数，然后提交到Hadoop（采用mapchain和reducechain）上即可解决该问题。

A map function. 对于map函数，采用hash算法，将hash值相同的数据交给同一个reduce task. 

2 reduce functions. 对于__第一个reduce函数__，采用HashMap统计出每个词出现的频率，对于__第二个reduce函数__，统计所有reduce task输出数据中的top k即可。

### 6. Other

公司一般不会自己写个程序进行计算，而是提交到自己核心的数据处理平台上计算，该平台的计算效率可能不如直接写程序高，但它具有__良好的扩展性和容错性__，而这才是企业最看重的。
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top k Frequency]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/07/25/big-data-Top-k-frequency/"/>
    <updated>2014-07-25T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/07/25/big-data-Top-k-frequency</id>
    <content type="html"><![CDATA[### Question 

[link](http://blog.csdn.net/v_JULY_v/article/details/6256463)

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。

> 假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

### Analysis

1. divide and conquer (for large input)

1. Query统计 (hash/trie)

1. 根据统计结果，找Top 10 (minheap / quickselect)

Be careful: 内存不能超过1G，10 million 条记录，每条记录是255Byte，很显然要占据2.375G内存. 

### Query统计

#### Option 1: HashMap

虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query 255Byte，因此我们可以考虑把他们都放进内存中去。

Hash Table绝对是我们优先的选择，因为Hash Table的查询速度非常的快，几乎是O(1)的时间复杂度。我们在O(N)的时间复杂度内完成了对该海量数据的处理。

#### Option 2: trie

> 这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n x le)（le表示单词的平准长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n x lg10)。所以总的时间复杂度，是O(n x le)与O(n x lg10)中较大的哪一个。

How to use Trie to calculate word frequency? 

> 在Trie的node节点中[添加count域后](http://blog.csdn.net/ohmygirl/article/details/7953814)，可以统计单词出现的次数。统计的方法就是在插入单词的时候，令相应的count域加1（初始化为0）. 

### 找Top 10

__Heap__. 

借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此到这里，我们的算法可以改进为这样，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。

查找目标元素的时间复杂度为 O(logK)。

### Quickselect

refer to another pose __[Fundamental] Quickselect__. 

### Conclusion

至此，算法就完全结束了。(这道题，我们不需要分治)。经过上述第一步、先用Hash表统计每个Query出现的次数，O（N）；然后第二步、采用堆数据结构找出Top 10，O(NlogK)。所以，我们最终的时间复杂度是：O（N） + O(N'logK)。
]]></content>
  </entry>
  
</feed>
