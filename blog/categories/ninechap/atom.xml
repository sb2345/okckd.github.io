<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ninechap | Shuatiblog.com]]></title>
  <link href="http://www.shuatiblog.com/blog/categories/ninechap/atom.xml" rel="self"/>
  <link href="http://www.shuatiblog.com/"/>
  <updated>2015-11-08T18:28:54-06:00</updated>
  <id>http://www.shuatiblog.com/</id>
  <author>
    <name><![CDATA[CodeMonkey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[NineChap System Design] Class 4.2: Search Engine ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/08/30/design-class4-2/"/>
    <updated>2015-08-30T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/08/30/design-class4-2</id>
    <content type="html"><![CDATA[]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[NineChap System Design] Class 4.1: Crawler ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/08/30/design-class4-1/"/>
    <updated>2015-08-30T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/08/30/design-class4-1</id>
    <content type="html"><![CDATA[# Overview

__KISS - Keep It Simple, Sweetie__.

In Today's lecture: 

1. write a crawler
1. thread-saft consumer & producer
1. GFS, BigTable and MapReduce
1. Top 10 keyword/anagram using MapReduce
1. Log analysis

# News Aggregator App

1. Info Collection

    crawler

1. Info retrieval: rank, search and recommend. 

    They are in fact, all related to __sorting__. 

<img class="middle" src="/assets/images/design-class4-News-Aggregator.png">

## Step 1, Info collection with crawler

### crawler code

Python:

    import urllib2
    
    # request
    url = "www.google.com"
    request = urllib2.Request(url)
    response = urllib2.urlopen(request)
    page = response.read()
    
    # save the file
    webFile = open('webpage.html', 'web')
    webFile.write(page)
    webFile.close()

### Network process

Use of __socket__. 

<img class="middle" src="/assets/images/design-class4-web-socket-1.png">

Socket is like the cellphone in the Call Center example. 

> __[socket](https://en.wikipedia.org/wiki/Network_socket)__ is an endpoint of an inter-process communication across a computer network. 

> Today, most communication between computers is based on the Internet Protocol; therefore most network sockets are __Internet sockets__.

What is socket? Where is it?

<img class="middle" src="/assets/images/design-class4-web-socket-2.png">

It's in-between __Application Layer__ (HTTP, FTP, DNS) and __Transport layer__ (UDP, TCP). 

Remembering that socket is like a cellphone. It is an __abstraction layer__, that hinds the complexity of lower layer, thus making it easier to sende data in application layer. 

### How is client connected to Server?

3-way handshake. Read __[Design] TCP 3-Way Handshake__

### HTML

[DOM tree](http://www.w3schools.com/js/js_htmldom.asp):

<img class="middle" src="/assets/images/html-dom-tree.gif">

> [Document Object Model](http://www.w3.org/TR/DOM-Level-2-Core/introduction.html) (DOM) is an application programming interface (API) for valid HTML and well-formed XML documents. It __defines the logical structure of documents__ and the way a document is accessed and manipulated. 

### How to crawler all the news

1. Go to index page
1. identify all the links (regex)

## Crawl more websites

### Simple design

1. use crawlers to find out all list pages
1. send the new lists to a Scheduler
1. Scheduler will use crawlers again, to crawl pages. 

<img class="middle" src="/assets/images/design-class4-cralwer-arch-1.png">

This design is bad, cuz there is crawler waste. How can we __reuse__ these crawlers???

### Adv design

Design crawler that can crawl __both list and pages__ information. 

> Look at our crawler: the text extraction logic and Regex for _abc.com_ and _bfe.com_ are totally different. However, they both share the same crawling techniques. 

So we pass in all info a crawler task needs. Like: 

<img class="middle" src="/assets/images/design-class4-cralwer-arch-2.png">

1. we gave __more priority to list pages than content pages__. Otherwise, your content get out of date soon. 

1. Type include both list/content and source info. 

1. status can be done, working, or new. 

1. timestamps helps us make sure each crawler runs every hour (let's say)

So when schedule __pick the next crawler task__ to run, it will choose __based on Priority__. However if the __timestamp (availableTime) is not yet reached__, the job won't be executed. 

If you crawler __runs until endTime__ and haven't finish, force finish it. We should also add __task created time__ to the info. 

### How to identify similar news?

Calculate the similarity between pages. More on this subject later. 

## How to design Scheduler?

<img class="middle" src="/assets/images/design-class4-cralwer-arch-3.png">

### Solution with Sleep

Define variables:

    taskTable<table> - store task lists
    pageTable<page> - store page contents
    
    task.url
    task.state = new/working/done
    task.type = list/page

code:

    while (true) {
        // get 1 task. If can't get, wait
        taskTable.lock();               // IMPORTANT
        newTask = taskTable.findOne(state == 'new')
        
        if (!newTask) {
            taskTable.unlock();         // IMPORTANT
            sleep(1000);                // IMPORTANT
            continue;
        }
        
        newTask.state = "working";
        taskTable.unlock();
        
        // execute the task, and insert to
        // either taskTable or pageTable
        newPage = Crawl(newTask.url);
        
        if (newTask.state === 'list') {
            // insert all urls to taskTable
            taskTable.lock();
            foreach (url in newPage) {
                taskTable.add(new task(url));
            }
            
            // mark the task as "done"
            newTask.state = "done";     // IMPORTANT
            taskTable.unlock();
        } else {
            // insert page content to pageTable
            pageTable.lock();
            pageTable.add(newPage.content());
            pageTable.unlock();
            
            // mark the task as "done"
            taskTable.lock();
            newTask.state = "done";     // IMPORTANT
            taskTable.unlock();
        }
    }

### Solution with Conditional Variable

What is Conditional Variable:

> [A condition variable](https://goo.gl/xOFXrY) is basically __a container of threads that are waiting on a certain condition__. 
>
> Monitors provide a mechanism for threads to temporarily give up exclusive access in order to wait for some condition to be met, before regaining exclusive access and resuming their task.

<img class="middle" src="/assets/images/design-class4-condition-variable.png">

Look at last 3 lines of code. __Before going to sleep, CV have to release the lock__, so that other threads can access the taskTable. 

Then CV goes to sleep. __Right after CV has been waken up__, it has to lock the mutex again. 

Solution w/ cv:

    while (true) {
        // get 1 task. If can't get, wait
        taskTable.lock();
        newTask = taskTable.findOne(state == 'new')
        
        if (!newTask) {
            taskTable.unlock();
            Cond_Wait(cond, taskTable);  // Modified
            continue;
        }
        
        newTask.state = "working";
        taskTable.unlock();
        
        // execute the task, and insert to
        // either taskTable or pageTable
        newPage = Crawl(newTask.url);
        
        if (newTask.state === 'list') {
            // insert all urls to taskTable
            taskTable.lock();
            foreach (url in newPage) {
                taskTable.add(new task(url));
                Cond_Signal(cond);       // Modified
            }
            
            // mark the task as "done"
            newTask.state = "done";
            taskTable.unlock();
        } else {
            // insert page content to pageTable
            pageTable.lock();
            pageTable.add(newPage.content());
            pageTable.unlock();
            
            // mark the task as "done"
            taskTable.lock();
            newTask.state = "done";
            taskTable.unlock();
        }
    }

Why Good: no need to busy-spin (example above have to always wait 1 second). So this solution is better. 

### Solution with Semaphore

This is better than Condition Variable, cuz it's easier to implement. And Semaphore not only locks thread, it also __can lock process__. 

CV locks on a certain condition. But Semaphore locks the numbers (of task, or resources etc). 

Semaphore implementation (fairly difficult, read for interest): 

    Wait(semaphore) {
        Lockf(semaphore);
        semaphore.value--;
        if (semaphore.value < 0) {
            semaphore.processWaitList.Add(this.process);
            Release(semaphore);
            Block(this.process);
        } else {
            Release(semaphore);
        }
    }

    Signal(semaphore) {
        Lock(semaphore);
        semaphore.value++;
        if (semaphore.value <= 0) {
            process = semaphore.processWaitList.pop();
            Wakeup(process);
        }
        Release(semaphore);
    }

Note that in Java and C++, Wait() == acquire() and Signal() == release(). Read more [Jenkov's post](http://tutorials.jenkov.com/java-util-concurrent/semaphore.html).

code w/ semaphore:

    while (true) {
        // get 1 task. If can't get, wait
        Wait(numberOfNewTask);           // Modified
        
        taskTable.lock();
        newTask = taskTable.findOne(state == 'new')
        newTask.state = "working";
        taskTable.unlock();
        
        // execute the task, and insert to
        // either taskTable or pageTable
        newPage = Crawl(newTask.url);
        
        if (newTask.state === 'list') {
            // insert all urls to taskTable
            taskTable.lock();
            foreach (url in newPage) {
                taskTable.add(new task(url));
                Signal(numberOfNewTask); // Modified
            }
            
            // mark the task as "done"
            newTask.state = "done";
            taskTable.unlock();
        } else {
            // insert page content to pageTable
            pageTable.lock();
            pageTable.add(newPage.content());
            pageTable.unlock();
            
            // mark the task as "done"
            taskTable.lock();
            newTask.state = "done";
            taskTable.unlock();
        }
    }

__What happens in Line 3 'Wait(numberOfNewTask)'__? Well, the programs checks on the numberOfNewTask (counter) variable, and:

1. If there is 1 or more tasks, just proceed.
1. If no tasks available, block itself and wait there. (Later someone will wake it up and it will resume). 

### Design an consumer-producer

Stay tuned for future post. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[NineChap System Design] Class 3.2: Web Service ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/08/28/design-class3-2/"/>
    <updated>2015-08-28T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/08/28/design-class3-2</id>
    <content type="html"><![CDATA[# Question 4

__fix MP3 problem__

The process of fetching a MP3 (from CDN):

<img class="middle" src="/assets/images/design-class3-client-request-mp3.png">

1. aquire MP3 link, and send request
1. send request to CDN
1. CDN receive request, find MP3
1. response to client
1. play the music

<img class="middle" src="/assets/images/design-class3-client-request-mp3-errors.png">

Question: in step 2, there's more Network error, but in step 4, there's no Network error, but Timeout. Why?

## Fix step 2, Network error

Problem is: MP3 url invalid. It actually comes from a failed CDN sever. 

Solution: fix the server.

## Fix step 3, CDN can't find MP3 

Problem associated with __Anti-Leech__. 

> __[a leech](https://en.wikipedia.org/wiki/Leech_(computing))__ is one who benefits, usually deliberately, from others' information or effort but does not offer anything in return. 
>
> Example: Wi-Fi leeches, Direct linking (or hot-linking) and In most P2P-networks, leeching is whose who download too much.

> __[Anti-Leech](https://answers.yahoo.com/question/index?qid=1006042926419)__ specializes in protecting file downloads and stopping bandwidth leeching. 

See that some P2P and leeching software will steal your url links, so the MP3 url expiration time is 5 minutes. 

So when CDN server's clock and web server's clock are not synchronized well, MP3 url can expire. 

Solution: every 10 minutes sync CDN clock with web server clock. 

## Fix step 4, Timeout error

Some MP3 are relatively large. Thus timeout. 

> MP3 performs better at higher bps, and aac(Advanced Audio Coding) works better at lower bps. 

Solution: 

1. compress MP3 to 48bps, or use aac format. So, play lower-rate music first, then switch automatically.

1. pre-load a music while previous is playing. 

1. optimize CDN

> __[CDN](https://en.wikipedia.org/wiki/Content_delivery_network)__ content delivery network is a large __distributed system of servers__ deployed in __multiple data centers__ across the Internet. 

> The goal of a CDN is to serve content to end-users with high availability and high performance.  

<img class="middle" src="/assets/images/design-class3-CDN.png">

Which CDN should client choose?

> Not DNS, but web server calculates which to choose. It can be calculated using IP distance, or ISP provider, but not accurate. 

> We can also use local desktop apps (in different locations) to ping CDN servers. This may violate user privacy, though. 

## Fix step 5, Fail to play

Problem: some files got wrong decoding. 

## Fix step 6, unkown error

Problem: some users close the page while MP3 loading. 

# Question 5

__fix player problem__

Problem: iOS device can never play Flash. 

Solution: develop HTML5 player. 

## 5.2 how to evaluate that you solved the problem

1. user complains

1. __important__: daily retention rate! 

We can't use daily active user, cuz it depends on marketing, competitors, and infrastructure changes. 

__One day retention rate__:

<img class="middle" src="/assets/images/design-class3-user-retention.png">

> Today’s visitor = {U1, U3, U7, U9, U10} 
>
> Tomorrow's visitor = {U2, U3, U9,}
>
> Today’s one day retention rate = 2/5

# Question 6 秒杀

## Design

Queue A and Queue B

<img class="middle" src="/assets/images/design-class3-miao-sha.png">

### Queue A

Many queues, each one locates on a individual web server or reverse proxy. It is mainly used to accept large amount of requests coming from the clients. 

Each machine may takes 10,000 or more requests per second. 

Queue A will redirect most requests to a static page (cached).

### Queue B

Queue B is a single machine, to aviod distributed problems. It takes in small amount of requests and decides results (eg. redirect to payment page). 

> Now, why do we need 2 queues, not 1?

> Think about a hospital. Queue A is the hospital lobby and security guard, and Queue B is the queue of patience. 

## How to reduce traffic

1. no image
1. cache more static pages
1. reverse proxy: batch sending the requests

Also, can use front-end javascript to prevent clicking. There are method to bypass, so we need to check IP or do some filtering. 

## How to keep it simple?

1. no DB: basic logic. But rmb to use a log file
1. no lock

## How to improve stability

1. use new server to do __Miao Sha__, in case of crash
1. asyc prcessing everything! Don't let other people wait, in case of crash. 

### How to defend hackers

1. IP address (to defent auto softwares), but it's easy for hackers to fake IP address
1. CAPTCHA 

> __[CAPTCHA](https://en.wikipedia.org/wiki/CAPTCHA)__ (an acronym for "Completely Automated Public Turing test to tell Computers and Humans Apart") is a type of challenge-response test used in computing to determine whether or not the user is human. 

## follow-up

How to design 12306 (support several million QPS)
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[NineChap System Design] Class 3.1: Web Service ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/08/28/design-class3-1/"/>
    <updated>2015-08-28T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/08/28/design-class3-1</id>
    <content type="html"><![CDATA[# Overview

Today we'll look at 6 examples of problems associated with Web Service:

1. how the internet works
1. DNS
1. Web server
1. Music Player
1. MP3 file
1. 秒杀

# Question 0

__how to solve raido-play failures__

    > Failure rate  = % user who can't listen to music properly

    > = # user who fail to plya one song / # total users
    
Misson: reduce failure rate.

## How does server identify a user?

<img class="middle" src="/assets/images/design-class3-http-request-overview.png">

> If a server uses __Cookie__ to identify unique users, the result might be > real users.

> However, if server uses __IP address__, the result might be < real users. 

## How to collect data for failure rate

### Version 1

Log: 

1. user send a log to server when it visits
1. user send another log after it plays a song
1. we can identify users who failed to play a song

<img class="middle" src="/assets/images/design-class3-server-failure-rate-1.png">

> In fact, everything should be logged, including play, pause, switch song, refresh etc.

### Version 2

User login are, in fact, __automatically logged__ when user visits. Thus user ONLY have to send log __after it plays music__. 

## Summary

1. define failure rate
1. user cookie to identify user
1. use log to collect failure data
1. analysis pattern of failure againt date/time

# Question 1

__the process of playing music__

<img class="middle" src="/assets/images/design-class3-web-browser-17-steps.png">

1. Prepare
2. Send DNS request
3. Prepare DNS reply
4. Send DNS reply
5. Process DNS reply

    -
    
6. Send webpage request
7. Prepare webpage reply
8. Send webpage reply
9. Process webpage
    
    -
    
10. Request music player
11. Prepare music player
12. Send music player
13. Process music player
    
    -
    
14. Request MP3 
15. Prepare MP3 
16. Send MP3 
17. Play MP3

What is process Music Play?

> Local browser will do rendering, flash decoding etc. If any point of this 17 steps went wrong, the music-play fails. 

Is there a system/browser default Music Play?

> HTML player is, but flash player is not. So the flash module have to be requested every time. 

## Real data: failure rate 20%

In practise, the real failure rate is 20%. Which is:

1. 8% DNS
1. 5% Web
1. 5% MP3
1. 2% Player

# Question 2

__fix DNS problem__

First of all, how to find out DNS failures? There are 2 ways. First way, help desk do it. Second way is to use the Desktop app to help detect the host address. 

## Step 1. HOSTs hijack

Some users' host file can modified by competitors. 

1. ping the website url
1. modify host file manually or by desktop app

## Step 2. ISP

Each ISP have different DNS service. Eg. CSTNET fails to update the latest DNS, after a server change. 

After this step, DNS failure rate fall from 8% to 1%. Why still 1%? Some companies bans music play in company web. 

# Question 3

__fix the web problem__

Highest failure rate:

1. 3pm office hour
1. 9pm highest bandwidth nation-wide

<img class="middle" src="/assets/images/design-class3-web-failure-graph.png">

## Solution 1, reverse proxy

Reverse proxy w/ more servers. Reverse proxy acts like a load balancer. 

<img class="middle" src="/assets/images/design-class3-reverse-proxly.png">

> __[Reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy)__ is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client as though they originated from the proxy server itself. 

[Common uses for a reverse proxy server](https://www.nginx.com/resources/glossary/reverse-proxy-server/) include:

> 1. Load balancing 

>    act as a “traffic cop,” sitting in front of your back-end servers and client requests. Try to __maximizes speed and capacity utilization__ while ensuring __no one server is overloaded__. 
    
>    If a server goes down, the load balancer redirects traffic to the remaining online servers.

> 1. Web acceleration 

>     can compress inbound and outbound data, as well as __cache commonly requested content__
    
>     also perform additional tasks such as SSL encryption to take __load off of your web servers__

> 1. Security and anonymity 

>     By intercepting requests headed for your back-end servers, a reverse proxy server protects their identities and acts as an additional defense against security attacks. 

## Solution 2, reduce size of web page

1. simplify javascript files
1. compress images (lower dpi)
1. merge large images to 1 image (less requests)
1. lazy loading (Pinterest uses it a lot)

## Solution 3, more cacheable pages

 __Change dynamic webpages to static pages__. The advantage of this is:

1. more search engine friendly.
1. more cache friendly.

### Summary on caching

Caching can happen at place Number 1, 2 and 3:

<img class="middle" src="/assets/images/design-class3-web-hosting-4-layers.png">

AT Number 4, we can add __more servers__. Number 3, __reverse proxy__. Number 2 is __caching within the ISP network__, which avoids requesting info again from backend. Number 1 is __front-end browser cache__. 

After this step, Web failure rate fall from 7% to 4%. Why still 4%? Well, these failure is mainly from the junk users created by marketing.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[NineChap System Design] Class 2.2: Database ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/08/26/design-class2-2/"/>
    <updated>2015-08-26T00:00:00-05:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/08/26/design-class2-2</id>
    <content type="html"><![CDATA[# Question 5

Continue from last post, now let's __support VIP services__. 

User could buy VIP services using his acccount balance. 

    class ProService {
        int userId;
        double money;
        long endTime;

        public addMoney(){}
        public buyPro(){}
    }

## Q5.1: System crash when purchasing VIP

Solution: __transaction with log__

    WriteLOG
    Transaction_1123: BEGIN
    money=20; endTime=16_07_2016

If system crash happens here, system will read the log, recover and roll back all original data. Try not to complete the transaction - just fail it. 

    WriteLOG
    Transaction_1123: BEGIN
    money=20; endTime=16_07_2016
    WriteLOG
    Transaction_1123: END
    money=10; endTime=16_08_2016

> What happens if system crash during writing the log? or during the rollback?

## Q5.2: dataset contains bad data

1. one user id have 2 corresponding pro-services information.
1. Shallow user: a pro-services info does not have corresponding user. 

Solution: have a checker class.

## Q5.3: simutaneous purchase by 2 users

Solution: lock. 

1. first process lock money & endTime.
1. Read money = 20
1. another process try to lock, but end up waiting (sleeping).
1. first process do the job, and release the lock. 
1. another process wakes up. 

> lock have time-out settings. It can be applied in distributed system as well. 

Question: does lock make your execution slow?

1. If another process is sleeping, CPU will be fully consumed by other process. So it won't impact. 

1. You can do some async processing, too. 

1. When you lock, try to lock only a small piece of code, not the entire method. In DB, lock only a row, not a table. 

1. Java [CAS](https://en.wikipedia.org/wiki/Compare-and-swap) (Compare and swap ) 

## Q5.4: Server crash

Solution: duplication. 

1. How many copies?

    Google did 3. Normally 2 in same data center, and 1 in another location. 
    
    Backup data normally is on another machine. But there's also [RAID](https://en.wikipedia.org/wiki/RAID) (redundant array of independent disks) which:
    
    > combines multiple physical disk drive components into a single logical unit for the purposes of __data redundancy, performance improvement__, or both. 

1. When does the copy happen? 

    Option 1 is __doing everyday nightly__. This is called a 'check point'.
    
    Option 2 is use another server to support __Shadow Redundancy__. All data from Machine 0 will be copied to Machine 1 WHILE it happens. The result is, Machine 1 is identical to Machine 0. If Machine 0 crashes, Machine 1 may be behind less than 1 second. 
    
    The way to duplicate is either re-play all the actions, or to read Machine 0's log and apply the new data. 

1. How to copy?

    User send data to 1 server, and from that server, pipeline. This ensures good usage of server bandwith, and serial transfer of data ensures low latency (several ms). 
    
    It's also possible to do tree-like transfer, but the above method is preferred cuz all machine consume same bandwidth.

1. What is log?

    It is actually 'checkpoint' + log. It allows you to rollback.

Data redundancy - Summary:

<img class="middle" src="/assets/images/design-class2-data-redundancy-1.png">

## Final note: Data inconsistency

Main sources of inconsistency comes from:

1. network fault
1. disk error

The disk eror is solved by __checksum__ (compare during disk writing). 

# Summary

__[ACID](https://en.wikipedia.org/wiki/ACID) (Atomicity, Consistency, Isolation, Durability)__  is a set of properties that guarantee that database transactions are processed reliably. 

1. __Atomicity: all or nothing__

    Q5.1: System crash when purchasing VIP
    
1. __Consistency__: validate according to defined rules

    Q5.2: dataset contains bad data
    
1. __Isolation__: independency between transactions __(lock)__

    Q5.3: simutaneous purchase by 2 users
    
1. __Durability__: stored permanently

    Q5.4: Server crash

<img class="middle" src="/assets/images/design-class2-summary.png">

Additional Questions:

1. design a user system (Netflix 2015)

Hint: table design, register, login/out, password check, find password. 

1. Design payment system (Yelp, BigCommerce 2015)

Hint: the question does not ask about table/ds design itself, but rather the problems associated with payment. Read about ACID principle. 
]]></content>
  </entry>
  
</feed>
