<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Bigdata | My Octopress Blog]]></title>
  <link href="http://okckd.github.io/blog/categories/bigdata/atom.xml" rel="self"/>
  <link href="http://okckd.github.io/"/>
  <updated>2014-08-04T23:30:32+08:00</updated>
  <id>http://okckd.github.io/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[BigData] Overview of Big Data Technology]]></title>
    <link href="http://okckd.github.io/blog/2014/08/04/Overview-Big-Data-Tech/"/>
    <updated>2014-08-04T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2014/08/04/Overview-Big-Data-Tech</id>
    <content type="html"><![CDATA[[link](https://datajobs.com/what-is-hadoop-and-nosql)

### Traditional RDBMS

Data is organized in a highly-structured manner, following the relational model. 

The need for the data to be well-structured actually becomes a substantial burden at extremely large volumes.

### NoSQL

A completely different framework of databases that allows for high-performance, agile processing of information at massive scale. 

NoSQL centers around the concept of distributed databases. 

It's horizontally scalable; as data continues to explode, just add more hardware to keep up.

### Hadoop

Hadoop is not a type of database, but rather a software ecosystem that allows for massively parallel computing.

Hadoop is [an open source implementation](http://www.javaworld.com/article/2077907/open-source-tools/mapreduce-programming-with-apache-hadoop.html?page=3) of the MapReduce programming model. Hadoop relies not on Google File System (GFS), but on its own Hadoop Distributed File System (HDFS). 

### MapReduce

An example of the Hadoop ecosystem is MapReduce. 

It's a computational model that basically takes intensive data processes and spreads the computation across a potentially endless number of servers. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[BigData] Hadoop Cluster]]></title>
    <link href="http://okckd.github.io/blog/2014/08/04/Hadoop-cluster/"/>
    <updated>2014-08-04T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2014/08/04/Hadoop-cluster</id>
    <content type="html"><![CDATA[[link](http://searchbusinessanalytics.techtarget.com/definition/Hadoop-cluster)

### 

A Hadoop cluster is __a special type of computational cluster__ designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment. 

Such clusters __run Hadoop's open source distributed processing software__ on low-cost commodity computers. 

Typically one machine in the cluster is designated as the __NameNode__ and another machine the as __JobTracker__; these are the masters. The rest of the machines in the cluster act as both __DataNode and TaskTracker__; these are the slaves.

Hadoop clusters are known for boosting the speed of data analysis applications. They also are highly scalable.

As of early 2013, Facebook was recognized as having the largest Hadoop cluster in the world. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[BigData] Find Top K Queries]]></title>
    <link href="http://okckd.github.io/blog/2014/07/25/Top-k-Queries/"/>
    <updated>2014-07-25T00:00:00+08:00</updated>
    <id>http://okckd.github.io/blog/2014/07/25/Top-k-Queries</id>
    <content type="html"><![CDATA[### Question 

[link](http://blog.csdn.net/v_JULY_v/article/details/6256463)

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。

> 假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

### Analysis

第一步：Query统计

第二步：根据统计结果，找Top 10

Be careful: 内存不能超过1G，10 million 条记录，每条记录是255Byte，很显然要占据2.375G内存. 

### Query统计

__HashMap__. 

虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query 255Byte，因此我们可以考虑把他们都放进内存中去。

Hash Table绝对是我们优先的选择，因为Hash Table的查询速度非常的快，几乎是O(1)的时间复杂度。我们在O(N)的时间复杂度内完成了对该海量数据的处理。

### 找Top 10

__Heap__. 

借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此到这里，我们的算法可以改进为这样，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。

查找目标元素的时间复杂度为 O(logK)。

### Conclusion

至此，算法就完全结束了，经过上述第一步、先用Hash表统计每个Query出现的次数，O（N）；然后第二步、采用堆数据结构找出Top 10，N*O（logK）。所以，我们最终的时间复杂度是：O（N） + N'*O（logK）。
]]></content>
  </entry>
  
</feed>
