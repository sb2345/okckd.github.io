<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: top k | Shuatiblog.com]]></title>
  <link href="http://www.shuatiblog.com/blog/categories/top-k/atom.xml" rel="self"/>
  <link href="http://www.shuatiblog.com/"/>
  <updated>2015-10-02T02:57:06+08:00</updated>
  <id>http://www.shuatiblog.com/</id>
  <author>
    <name><![CDATA[CodeMonkey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top k Frequency (hands-on) ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/01/09/big-data-top-k-frequency-3/"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/01/09/big-data-top-k-frequency-3</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/a/3262855)

> The input is an endless stream of English words or phrases (we refer them as tokens).

> Output top N tokens we have seen so far (from all the tokens we have seen!) 

### Analysis

We will discuss the following details of implementation and optimization. 

1. String into Integer
2. Data Storage
3. Process Incoming Streams
4. Save result

#### 1. String into Integer

This is a nice trick that improves eficiency a lot. 

> Though there is almost infinite possible words on the Internet, but after accumulating a large set of words, the possibility of finding new words becomes lower and lower.

> We have already found 4 million different words, and assigned a unique ID for each. This is important, because sorting and comparisons on integers is __much much faster__ than on strings.

#### 2. Data Storage

> The system keeps archive data for every token. Basically it's pairs of (Token, Frequency). 

> However, the table that stores the data would be so huge such that we have to partition the table physically. One partition scheme is __based on ngrams__ of the token. If the token is a single word, it is 1gram. If the token is two-word phrase, it is 2gram. 

Of course we can also divide the data by the hash value. For information on __ngrams__, read __[Design] Terminology: n-gram__. 

#### 3. Process Incoming Streams

> The system will absorbs incoming sentences until memory becomes fully utilized (Ya, we need a MemoryManager). After taking N sentences and storing in memory, the system pauses, and starts tokenize each sentence into words and phrases. Each token (word or phrase) is counted. 

This data processing logic runs as a process under Memory-Manager. The next part is another processing running concurrently. 

#### 4. Save result

> Meanwhile, there will be another process that is activated once it finds any disk file generated by the system, then start merging it. Since the disk file is sorted, merging would take __a similar process like merge sort__. 

There is [some more steps](http://stackoverflow.com/a/3262855) afterwards, but they're trivial. I have listed out the basic steps for processing large stream of incoming data (as string), and how to find out the Top K keywords. 

I suggest you read previous __[Design] Big Data - Top k Frequency__ posts before reading this. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Real Time Top k ]]></title>
    <link href="http://www.shuatiblog.com/blog/2015/01/09/big-data-real-time-top-k/"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>http://www.shuatiblog.com/blog/2015/01/09/big-data-real-time-top-k</id>
    <content type="html"><![CDATA[### Question 

[link](http://stackoverflow.com/questions/10189685/realtime-tracking-of-top-100-twitter-words-per-min-hour-day)

> Given a continuous twitter feed, design an algorithm to return the 100 most
frequent words used at this minute, this hour and this day. 

### Analysis

This is a frequent and useful problem for companies like Google and Twitter. 

The first solution below is __an approximation method__ which select keywords that occur more than a certain threshold. 

The second solution is __more accurate__ but RAM-intensive. 

### Lossy Count (used to get an inaccurate trend) 

__Solution 1 is a modified version of [Lossy Count](http://stackoverflow.com/a/8033083)__. The detailed steps are explained [here](http://stackoverflow.com/a/3260905): 

> Start with an empty map (red-black tree). The keys will be search terms, and the values will be a counter for the term. 
>
> 1. Look at each item in the stream.
>
> 1. If the term exists in the map, increment the associated counter.
>
> 1. Otherwise, if the map has fewer candidates than you're looking for, add it to the map with a count of one.
>
> 1. However, if the map is "full", decrement the counter in each entry. If any counter reaches zero during this process, remove it from the map.

[This slide show](http://www.cse.ust.hk/vldb2002/VLDB2002-proceedings/slides/S10P03slides.pdf) explains __Lossy Count__, which is to divide input data into chunks. Then count elements and decrease counter by 1 after each chunk. 

__Note that the result is NOT the top frequency items__. Instead, the final results are __order-dependent__, giving heavier weight to the counts processed last. It maybe helpful in some cases, cuz we want to check the latest trend. However, if we want more accurate top keywords for all data, we will __do a second pass over the log data__. 

Now let's discuss the threshold. Use "aaabcd" and map size = 2 as example. 'a' will be inserted into map with occurance = 3. Then 'b' is inserted, and removed. 'c' is inserted, and removed. 'd' is inserted. Since we always decrease 1 at each step, 'a' should only have occurance of 1 at the end. As explained [here](http://stackoverflow.com/a/3260905): 

> If we limit the map to 99 entries, we are guaranteed to find any term that occurs more than 1/(1 + 99) (1%) of the time. 

We change the size of the map to change the threshold. The occurance of in the final result does not matter. 

### Solution 2

The lossy count does not actually produce the hourly, daily and monthly result accurately. Solution 2 will discuss how we deal with retiring old data in an accurate way. 

Suggested by [this answer](http://stackoverflow.com/a/3260768), __we keep a 30-day list for each keyword__, that counts the daily occurance. This list is FIFO. When we remove and insert a new counter value, we update monthly total. 

Alaternatively, [this answer](http://stackoverflow.com/a/10190836) suggests keeping 1440 (24 * 60) HashMaps, each storing the information for one minute. __And another 2 HashMap for the rolling total for the past hour, and past day__. 

> You need an array of 1440 (24*60) word+count hash maps organized the way that you describe; these are your minute-by-minute counts. You need two additional hash maps - for the rolling total of the hour and the day.

> Define two operations on hash maps - add and subtract, with the semantic of merging counts of identical words, and removing words when their count drops to zero.

> Each minute you start a new hash map, and update counts from the feed. At the end of the minute, you place that hash map into the array for the current minute, add it to the rolling total for the hour and for the day, and then subtract the hash map of an hour ago from the hourly running total, and subtract the hash map of 24 hours ago from the daily running total.

This is a very good solution, which I would recommend as the standard solution to this "Real Time Top k" problem. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[CC150v4] 20.6 Top Million from Billion ]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/09/10/top-million-from-billion/"/>
    <updated>2014-09-10T00:00:00+08:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/09/10/top-million-from-billion</id>
    <content type="html"><![CDATA[### Question

> Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. 

> Assume that the computer memory can hold all one billion numbers. 

### Solution

There're enough discussion on __Top K problems__ so far in this blog. The suggest solutions is: 

1. Sort

1. Min Heap, O(n logm) time. 

1. Quickselect algorithm. O(n) time. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Big Data - Top k Frequency]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/07/25/big-data-Top-k-frequency/"/>
    <updated>2014-07-25T00:00:00+08:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/07/25/big-data-Top-k-frequency</id>
    <content type="html"><![CDATA[### Question 

[link](http://blog.csdn.net/v_JULY_v/article/details/6256463)

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。

> 假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

### Analysis

1. divide and conquer (for large input)

1. Query统计 (hash/trie)

1. 根据统计结果，找Top 10 (minheap / quickselect)

Be careful: 内存不能超过1G，10 million 条记录，每条记录是255Byte，很显然要占据2.375G内存. 

### Query统计

#### Option 1: HashMap

虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query 255Byte，因此我们可以考虑把他们都放进内存中去。

Hash Table绝对是我们优先的选择，因为Hash Table的查询速度非常的快，几乎是O(1)的时间复杂度。我们在O(N)的时间复杂度内完成了对该海量数据的处理。

#### Option 2: trie

> 这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n x le)（le表示单词的平准长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n x lg10)。所以总的时间复杂度，是O(n x le)与O(n x lg10)中较大的哪一个。

How to use Trie to calculate word frequency? 

> 在Trie的node节点中[添加count域后](http://blog.csdn.net/ohmygirl/article/details/7953814)，可以统计单词出现的次数。统计的方法就是在插入单词的时候，令相应的count域加1（初始化为0）. 

### 找Top 10

__Heap__. 

借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此到这里，我们的算法可以改进为这样，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。

查找目标元素的时间复杂度为 O(logK)。

### Conclusion

至此，算法就完全结束了，经过上述第一步、先用Hash表统计每个Query出现的次数，O（N）；然后第二步、采用堆数据结构找出Top 10，O(NlogK)。所以，我们最终的时间复杂度是：O（N） + O(N'logK)。

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Question] Top k Element (Quickselect)]]></title>
    <link href="http://www.shuatiblog.com/blog/2014/07/25/Top-k-Element/"/>
    <updated>2014-07-25T00:00:00+08:00</updated>
    <id>http://www.shuatiblog.com/blog/2014/07/25/Top-k-Element</id>
    <content type="html"><![CDATA[### Question 

[link](http://www.geeksforgeeks.org/k-largestor-smallest-elements-in-an-array/)

> Find Top k smallest element in an array. 

### Analysis

There're 2 solutions. 

First solution, __use a max-heap__. O(nlgk) time complexity.

Second solution is called __[quickselect](http://www.geekviewpoint.com/java/search/quickselect)__, a type of [selection algorithm](http://en.wikipedia.org/wiki/Selection_algorithm) that's based on quicksort. It's averaging O(n) time, but O(n^2) if pivot selection is poor. The code is posted below. There's also a similar [iterative solution](http://blog.teamleadnet.com/2012/07/quick-select-algorithm-find-kth-element.html). 

To [further optimize this](http://www.isnowfy.com/top-k-number/), we can change the pivot selection method by dividing into k group and find median of each. This is called [Median of medians algorithm](http://en.wikipedia.org/wiki/Median_of_medians). The worst case is O(n) time. And this is the best solution for "Top k" questions. 

### Why quickselect is O(n) time?

It's a very good question to ask. Why O(n)?

Well think about it. Let's assume you always find the pivot that makes you eliminate half of the input. 

__The first run, you would read n elements__. Second time you read half of n, and third time, quarter of n. In the end, you read n + n/2 + n/4 + ... = 2n times. 

Compared to the Heap method to find top K, quick select has its advantage. __Heap top K take O(n lgK) time__. So __when K is pretty large__, quick select might be an better solution. 

### Code 

__quickselect__

	public static void quickSelect1(int[] list, int k) {
		selectHelper1(list, 0, list.length - 1, k);
	}

	public static void selectHelper1(int[] list, int left, int right, int k) {
		int pivotIndex = partition(list, left, right);
		if (pivotIndex == k) {
			return;
		} else if (k < pivotIndex) {
			selectHelper1(list, left, pivotIndex - 1, k);
		} else {
			selectHelper1(list, pivotIndex + 1, right, k);
		}
	}

	private static int partition(int[] list, int left, int right) {
		int pivot = left + (right - left) / 2;
		swap(list, right, pivot);
		for (int i = left; i < right; i++) {
			if (list[i] < list[right]) {
				swap(list, i, left);
				left++;
			}
		}
		swap(list, left, right);
		return left;
	}

__quickselect, iteratively__

	public static int quickSelect2(int[] arr, int k) {
		if (arr == null || arr.length <= k)
			throw new Error();
		int from = 0, to = arr.length - 1;
		// if from == to we reached the kth element
		while (from < to) {
			int r = from, w = to;
			int mid = arr[(r + w) / 2];
			// stop if the reader and writer meets
			while (r < w) {
				if (arr[r] >= mid) { // put the large values at the end
					swap(arr, w, r);
					w--;
				} else { // the value is smaller than the pivot, skip
					r++;
				}
			}
			// if we stepped up (r++) we need to step one down
			if (arr[r] > mid)
				r--;
			// the r pointer is on the end of the first k elements
			if (k <= r) {
				to = r;
			} else {
				from = r + 1;
			}
		}
		return arr[k];
	}
]]></content>
  </entry>
  
</feed>
